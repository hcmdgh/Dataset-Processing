{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1mbook\u001b[0m={\n",
       "    num_nodes=40402,\n",
       "    y=[40402],\n",
       "    train_mask=[40402],\n",
       "    test_mask=[40402]\n",
       "  },\n",
       "  \u001b[1mfilm\u001b[0m={ num_nodes=19427 },\n",
       "  \u001b[1mmusic\u001b[0m={ num_nodes=82351 },\n",
       "  \u001b[1msports\u001b[0m={ num_nodes=1025 },\n",
       "  \u001b[1mpeople\u001b[0m={ num_nodes=17641 },\n",
       "  \u001b[1mlocation\u001b[0m={ num_nodes=9368 },\n",
       "  \u001b[1morganization\u001b[0m={ num_nodes=2731 },\n",
       "  \u001b[1mbusiness\u001b[0m={ num_nodes=7153 },\n",
       "  \u001b[1m(book, and, book)\u001b[0m={ edge_index=[2, 202674] },\n",
       "  \u001b[1m(book, to, film)\u001b[0m={ edge_index=[2, 38299] },\n",
       "  \u001b[1m(book, on, sports)\u001b[0m={ edge_index=[2, 6615] },\n",
       "  \u001b[1m(book, on, location)\u001b[0m={ edge_index=[2, 26921] },\n",
       "  \u001b[1m(book, about, organization)\u001b[0m={ edge_index=[2, 21900] },\n",
       "  \u001b[1m(film, and, film)\u001b[0m={ edge_index=[2, 87838] },\n",
       "  \u001b[1m(music, in, book)\u001b[0m={ edge_index=[2, 31486] },\n",
       "  \u001b[1m(music, in, film)\u001b[0m={ edge_index=[2, 11291] },\n",
       "  \u001b[1m(music, and, music)\u001b[0m={ edge_index=[2, 283670] },\n",
       "  \u001b[1m(music, for, sports)\u001b[0m={ edge_index=[2, 8975] },\n",
       "  \u001b[1m(music, on, location)\u001b[0m={ edge_index=[2, 42915] },\n",
       "  \u001b[1m(sports, in, film)\u001b[0m={ edge_index=[2, 6763] },\n",
       "  \u001b[1m(sports, and, sports)\u001b[0m={ edge_index=[2, 1290] },\n",
       "  \u001b[1m(sports, on, location)\u001b[0m={ edge_index=[2, 656] },\n",
       "  \u001b[1m(people, to, book)\u001b[0m={ edge_index=[2, 35587] },\n",
       "  \u001b[1m(people, to, film)\u001b[0m={ edge_index=[2, 17604] },\n",
       "  \u001b[1m(people, to, music)\u001b[0m={ edge_index=[2, 10948] },\n",
       "  \u001b[1m(people, to, sports)\u001b[0m={ edge_index=[2, 14850] },\n",
       "  \u001b[1m(people, and, people)\u001b[0m={ edge_index=[2, 22813] },\n",
       "  \u001b[1m(people, on, location)\u001b[0m={ edge_index=[2, 15134] },\n",
       "  \u001b[1m(people, in, organization)\u001b[0m={ edge_index=[2, 2215] },\n",
       "  \u001b[1m(people, in, business)\u001b[0m={ edge_index=[2, 5378] },\n",
       "  \u001b[1m(location, in, film)\u001b[0m={ edge_index=[2, 21299] },\n",
       "  \u001b[1m(location, and, location)\u001b[0m={ edge_index=[2, 47817] },\n",
       "  \u001b[1m(organization, in, film)\u001b[0m={ edge_index=[2, 13128] },\n",
       "  \u001b[1m(organization, to, music)\u001b[0m={ edge_index=[2, 10702] },\n",
       "  \u001b[1m(organization, to, sports)\u001b[0m={ edge_index=[2, 559] },\n",
       "  \u001b[1m(organization, on, location)\u001b[0m={ edge_index=[2, 2696] },\n",
       "  \u001b[1m(organization, and, organization)\u001b[0m={ edge_index=[2, 1101] },\n",
       "  \u001b[1m(organization, for, business)\u001b[0m={ edge_index=[2, 1073] },\n",
       "  \u001b[1m(business, about, book)\u001b[0m={ edge_index=[2, 18625] },\n",
       "  \u001b[1m(business, about, film)\u001b[0m={ edge_index=[2, 8397] },\n",
       "  \u001b[1m(business, about, music)\u001b[0m={ edge_index=[2, 24764] },\n",
       "  \u001b[1m(business, about, sports)\u001b[0m={ edge_index=[2, 610] },\n",
       "  \u001b[1m(business, on, location)\u001b[0m={ edge_index=[2, 6647] },\n",
       "  \u001b[1m(business, and, business)\u001b[0m={ edge_index=[2, 4448] }\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import HeteroData \n",
    "from collections import defaultdict \n",
    "import torch \n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# node_types = {0: 'paper', 1, 'author', ...}\n",
    "# edge_types = {0: ('paper', 'cite', 'paper'), ...}\n",
    "if True:\n",
    "    with open('/mnt1/genghao/dataset/HGB/raw/Freebase/info.dat', 'r') as f:  # `info.dat`\n",
    "        info = f.read().split('\\n')\n",
    "    start = info.index('TYPE\\tMEANING') + 1\n",
    "    end = info[start:].index('')\n",
    "    n_types = [v.split('\\t\\t') for v in info[start:start + end]]\n",
    "    n_types = {int(k): v.lower() for k, v in n_types}\n",
    "\n",
    "    e_types = {}\n",
    "    start = info.index('LINK\\tSTART\\tEND\\tMEANING') + 1\n",
    "    end = info[start:].index('')\n",
    "    for key, row in enumerate(info[start:start + end]):\n",
    "        row = row.split('\\t')[1:]\n",
    "        src, dst, rel = [v for v in row if v != '']\n",
    "        src, dst = n_types[int(src)], n_types[int(dst)]\n",
    "        rel = rel.split('-')[1]\n",
    "        e_types[key] = (src, rel, dst)\n",
    "\n",
    "# Extract node information:\n",
    "mapping_dict = {}  # Maps global node indices to local ones.\n",
    "x_dict = defaultdict(list)\n",
    "num_nodes_dict = defaultdict(lambda: 0)\n",
    "with open('/mnt1/genghao/dataset/HGB/raw/Freebase/node.dat', 'r') as f:  # `node.dat`\n",
    "    xs = [v.split('\\t') for v in f.read().split('\\n')[:-1]]\n",
    "for x in xs:\n",
    "    n_id, n_type = int(x[0]), n_types[int(x[2])]\n",
    "    mapping_dict[n_id] = num_nodes_dict[n_type]\n",
    "    num_nodes_dict[n_type] += 1\n",
    "    if len(x) >= 4:  # Extract features (in case they are given).\n",
    "        x_dict[n_type].append([float(v) for v in x[3].split(',')])\n",
    "for n_type in n_types.values():\n",
    "    if len(x_dict[n_type]) == 0:\n",
    "        data[n_type].num_nodes = num_nodes_dict[n_type]\n",
    "    else:\n",
    "        data[n_type].x = torch.tensor(x_dict[n_type])\n",
    "\n",
    "edge_index_dict = defaultdict(list)\n",
    "edge_weight_dict = defaultdict(list)\n",
    "with open('/mnt1/genghao/dataset/HGB/raw/Freebase/link.dat', 'r') as f:  # `link.dat`\n",
    "    edges = [v.split('\\t') for v in f.read().split('\\n')[:-1]]\n",
    "for src, dst, rel, weight in edges:\n",
    "    e_type = e_types[int(rel)]\n",
    "    src, dst = mapping_dict[int(src)], mapping_dict[int(dst)]\n",
    "    edge_index_dict[e_type].append([src, dst])\n",
    "    edge_weight_dict[e_type].append(float(weight))\n",
    "for e_type in e_types.values():\n",
    "    edge_index = torch.tensor(edge_index_dict[e_type])\n",
    "    edge_weight = torch.tensor(edge_weight_dict[e_type])\n",
    "    data[e_type].edge_index = edge_index.t().contiguous()\n",
    "    # Only add \"weighted\" edgel to the graph:\n",
    "    if not torch.allclose(edge_weight, torch.ones_like(edge_weight)):\n",
    "        data[e_type].edge_weight = edge_weight\n",
    "\n",
    "# Node classification:\n",
    "if True:\n",
    "    with open('/mnt1/genghao/dataset/HGB/raw/Freebase/label.dat', 'r') as f:  # `label.dat`\n",
    "        train_ys = [v.split('\\t') for v in f.read().split('\\n')[:-1]]\n",
    "    with open('/mnt1/genghao/dataset/HGB/raw/Freebase/label.dat.test', 'r') as f:  # `label.dat.test`\n",
    "        test_ys = [v.split('\\t') for v in f.read().split('\\n')[:-1]]\n",
    "    for y in train_ys:\n",
    "        n_id, n_type = mapping_dict[int(y[0])], n_types[int(y[2])]\n",
    "\n",
    "        if not hasattr(data[n_type], 'y'):\n",
    "            num_nodes = data[n_type].num_nodes\n",
    "            if False:  # multi-label\n",
    "                data[n_type].y = torch.zeros((num_nodes, num_classes))\n",
    "            else:\n",
    "                data[n_type].y = torch.full((num_nodes, ), -1).long()\n",
    "            data[n_type].train_mask = torch.zeros(num_nodes).bool()\n",
    "            data[n_type].test_mask = torch.zeros(num_nodes).bool()\n",
    "\n",
    "        if False:  # multi-label\n",
    "            for v in y[3].split(','):\n",
    "                data[n_type].y[n_id, int(v)] = 1\n",
    "        else:\n",
    "            data[n_type].y[n_id] = int(y[3])\n",
    "        data[n_type].train_mask[n_id] = True\n",
    "        \n",
    "    print(len(test_ys))\n",
    "        \n",
    "    for y in test_ys:\n",
    "        n_id, n_type = mapping_dict[int(y[0])], n_types[int(y[2])]\n",
    "        if False:  # multi-label\n",
    "            for v in y[3].split(','):\n",
    "                data[n_type].y[n_id, int(v)] = 1\n",
    "        else:\n",
    "            data[n_type].y[n_id] = int(y[3])\n",
    "        data[n_type].test_mask[n_id] = True\n",
    "        \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1863)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.y_dict['book'] \n",
    "\n",
    "(y > 0).float().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0591), tensor(0.1378))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask = data.train_mask_dict['book']\n",
    "test_mask = data.test_mask_dict['book'] \n",
    "assert (train_mask & test_mask).float().sum() == 0 \n",
    "\n",
    "train_mask.float().mean(), test_mask.float().mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max() + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 40402,\n",
       " 'film': 19427,\n",
       " 'music': 82351,\n",
       " 'sports': 1025,\n",
       " 'people': 17641,\n",
       " 'location': 9368,\n",
       " 'organization': 2731,\n",
       " 'business': 7153}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_nodes_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('/mnt1/genghao/dataset/HGB/processed/Freebase_hg.dict.pkl', 'wb') as fp: \n",
    "    pickle.dump(\n",
    "        dict(\n",
    "            num_nodes_dict = dict(data.num_nodes_dict), \n",
    "            edge_index_dict = dict(data.edge_index_dict),\n",
    "            book_label = data.y_dict['book'], \n",
    "            book_train_mask = data.train_mask_dict['book'], \n",
    "            book_test_mask = data.test_mask_dict['book'], \n",
    "        ), \n",
    "        fp, \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
