{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26128it [00:07, 3382.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4057, 334), (14328, 4231), (7723, 50), 20)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "import os \n",
    "import json \n",
    "import numpy as np \n",
    "import traceback \n",
    "from tqdm import tqdm \n",
    "\n",
    "with open(os.path.expanduser('~/dataset/HGB/raw/DBLP/node.dat'), 'r', encoding='utf-8') as fp:\n",
    "    reader = csv.DictReader(fp, fieldnames=['nid', 'name', 'node_type', 'feat'], delimiter='\\t') \n",
    "\n",
    "    author_feat_list = [] \n",
    "    paper_feat_list = [] \n",
    "    term_feat_list = [] \n",
    "    venue_feat_list = [] \n",
    "    \n",
    "    nid_map: dict[int, tuple[str, int]] = dict() \n",
    "\n",
    "    for row in tqdm(reader):\n",
    "        nid = int(row['nid']) \n",
    "        name = row['name'].strip() \n",
    "        ntype = int(row['node_type']) \n",
    "\n",
    "        try:\n",
    "            feat_str = row['feat'].strip() \n",
    "            feat = np.array(json.loads(f\"[{feat_str}]\"), dtype=np.float32)  \n",
    "        except Exception:\n",
    "            feat = None  \n",
    "            \n",
    "        if ntype == 0: \n",
    "            nid_map[nid] = ('author', len(author_feat_list)) \n",
    "            author_feat_list.append(feat)\n",
    "        elif ntype == 1: \n",
    "            nid_map[nid] = ('paper', len(paper_feat_list)) \n",
    "            paper_feat_list.append(feat)\n",
    "        elif ntype == 2: \n",
    "            nid_map[nid] = ('term', len(term_feat_list)) \n",
    "            term_feat_list.append(feat) \n",
    "        elif ntype == 3: \n",
    "            nid_map[nid] = ('venue', len(venue_feat_list)) \n",
    "            venue_feat_list.append(feat)\n",
    "        else:\n",
    "            raise AssertionError \n",
    "            \n",
    "author_feat = np.stack(author_feat_list) \n",
    "paper_feat = np.stack(paper_feat_list) \n",
    "term_feat = np.stack(term_feat_list) \n",
    "assert all(x is None for x in venue_feat_list) \n",
    "    \n",
    "author_feat.shape, \\\n",
    "paper_feat.shape, \\\n",
    "term_feat.shape, \\\n",
    "len(venue_feat_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "239566it [00:00, 302345.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2, 19645), (2, 14328), (2, 85810))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.expanduser('~/dataset/HGB/raw/DBLP/link.dat'), 'r', encoding='utf-8') as fp:\n",
    "    reader = csv.DictReader(fp, fieldnames=['src_nid', 'dest_nid', 'etype', 'score'], delimiter='\\t') \n",
    "    \n",
    "    author_paper_edge_list = []\n",
    "    paper_term_edge_list = []\n",
    "    paper_venue_edge_list = []\n",
    "    paper_author_edge_list = []\n",
    "    term_paper_edge_list = []\n",
    "    venue_paper_edge_list = []\n",
    "    \n",
    "    for row in tqdm(reader):\n",
    "        src_nid = int(row['src_nid'])\n",
    "        dest_nid = int(row['dest_nid']) \n",
    "        etype = int(row['etype'])\n",
    "        score = float(row['score']) \n",
    "        assert score == 1. \n",
    "        \n",
    "        if etype == 0:\n",
    "            assert nid_map[src_nid][0] == 'author' and nid_map[dest_nid][0] == 'paper'\n",
    "            author_paper_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        elif etype == 1:\n",
    "            assert nid_map[src_nid][0] == 'paper' and nid_map[dest_nid][0] == 'term'\n",
    "            paper_term_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        elif etype == 2:\n",
    "            assert nid_map[src_nid][0] == 'paper' and nid_map[dest_nid][0] == 'venue'\n",
    "            paper_venue_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        elif etype == 3:\n",
    "            assert nid_map[src_nid][0] == 'paper' and nid_map[dest_nid][0] == 'author'\n",
    "            paper_author_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        elif etype == 4:\n",
    "            assert nid_map[src_nid][0] == 'term' and nid_map[dest_nid][0] == 'paper'\n",
    "            term_paper_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        elif etype == 5:\n",
    "            assert nid_map[src_nid][0] == 'venue' and nid_map[dest_nid][0] == 'paper'\n",
    "            venue_paper_edge_list.append((nid_map[src_nid][1], nid_map[dest_nid][1])) \n",
    "        else:\n",
    "            raise AssertionError \n",
    "    \n",
    "author_paper_edge_index = np.array(author_paper_edge_list, dtype=np.int64).T \n",
    "paper_term_edge_index = np.array(paper_term_edge_list, dtype=np.int64).T \n",
    "paper_venue_edge_index = np.array(paper_venue_edge_list, dtype=np.int64).T \n",
    "paper_author_edge_index = np.array(paper_author_edge_list, dtype=np.int64).T \n",
    "term_paper_edge_index = np.array(term_paper_edge_list, dtype=np.int64).T \n",
    "venue_paper_edge_index = np.array(venue_paper_edge_list, dtype=np.int64).T  \n",
    "\n",
    "paper_author_edge_index.shape, \\\n",
    "paper_venue_edge_index.shape, \\\n",
    "paper_term_edge_index.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1217it [00:00, 63388.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4057,), 1217, 1217)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.expanduser('~/dataset/HGB/raw/DBLP/label.dat'), 'r', encoding='utf-8') as fp:\n",
    "    reader = csv.DictReader(fp, fieldnames=['author_nid', 'author_name', 'node_type', 'author_label'], delimiter='\\t') \n",
    "    \n",
    "    author_label_arr = np.full([len(author_feat)], fill_value=-1, dtype=np.int64)   \n",
    "    author_train_mask = np.zeros(len(author_feat), dtype=bool) \n",
    "    \n",
    "    for row in tqdm(reader):\n",
    "        author_nid = int(row['author_nid']) \n",
    "        author_name = row['author_name'].strip() \n",
    "        ntype = int(row['node_type']) \n",
    "        assert ntype == 0 \n",
    "        author_label = int(row['author_label'])\n",
    "        assert author_label in [0, 1, 2, 3] \n",
    "        \n",
    "        assert nid_map[author_nid][0] == 'author'\n",
    "        assert author_label_arr[nid_map[author_nid][1]] == -1 \n",
    "        author_label_arr[nid_map[author_nid][1]] = author_label\n",
    "        author_train_mask[nid_map[author_nid][1]] = True \n",
    "    \n",
    "author_label_arr.shape, \\\n",
    "np.sum(author_label_arr > -1), \\\n",
    "np.sum(author_train_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2840it [00:00, 75693.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4057,), 4057)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.expanduser('~/dataset/HGB/raw/DBLP/label.dat.test'), 'r', encoding='utf-8') as fp:\n",
    "    reader = csv.DictReader(fp, fieldnames=['author_nid', 'author_name', 'node_type', 'author_label'], delimiter='\\t') \n",
    "    \n",
    "    for row in tqdm(reader):\n",
    "        author_nid = int(row['author_nid']) \n",
    "        author_name = row['author_name'].strip() \n",
    "        ntype = int(row['node_type']) \n",
    "        assert ntype == 0 \n",
    "        author_label = int(row['author_label'])\n",
    "        assert author_label in [0, 1, 2, 3] \n",
    "        \n",
    "        assert nid_map[author_nid][0] == 'author'\n",
    "        assert author_label_arr[nid_map[author_nid][1]] == -1 \n",
    "        author_label_arr[nid_map[author_nid][1]] = author_label\n",
    "    \n",
    "author_label_arr.shape, \\\n",
    "np.sum(author_label_arr > -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch \n",
    "\n",
    "with open(os.path.expanduser('~/dataset/HGB/processed/DBLP_hg.dict.pkl'), 'wb') as fp:\n",
    "    pickle.dump(\n",
    "        dict(\n",
    "            node_feat_dict = dict(\n",
    "                author = torch.tensor(author_feat, dtype=torch.float32), \n",
    "                paper = torch.tensor(paper_feat, dtype=torch.float32), \n",
    "                term = torch.tensor(term_feat, dtype=torch.float32), \n",
    "            ), \n",
    "            num_nodes_dict = dict(\n",
    "                author = len(author_feat), \n",
    "                paper = len(paper_feat), \n",
    "                term = len(term_feat), \n",
    "                venue = len(venue_feat_list), \n",
    "            ),\n",
    "            edge_index_dict = {\n",
    "                ('author', 'AP', 'paper'): torch.tensor(author_paper_edge_index, dtype=torch.int64),\n",
    "                ('paper', 'PT', 'term'): torch.tensor(paper_term_edge_index, dtype=torch.int64),\n",
    "                ('paper', 'PV', 'venue'): torch.tensor(paper_venue_edge_index, dtype=torch.int64),\n",
    "                ('paper', 'PA', 'author'): torch.tensor(paper_author_edge_index, dtype=torch.int64),\n",
    "                ('term', 'TP', 'paper'): torch.tensor(term_paper_edge_index, dtype=torch.int64),\n",
    "                ('venue', 'VP', 'paper'): torch.tensor(venue_paper_edge_index, dtype=torch.int64),\n",
    "            },\n",
    "            author_label = torch.tensor(author_label_arr, dtype=torch.int64), \n",
    "            author_train_mask = torch.tensor(author_train_mask, dtype=torch.bool),  \n",
    "            author_test_mask = torch.tensor(~author_train_mask, dtype=torch.bool), \n",
    "        ), \n",
    "        fp, \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
